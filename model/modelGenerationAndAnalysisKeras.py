# -*- coding: utf-8 -*-
"""AnalyzingScores.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SWx4qMJLMEPq_XERa2YEsxmdkwwTGq_a
"""

#used google colab for processing

# from google.colab import drive
# drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import os
import nltk
from keras.models import Sequential
from keras.layers import LSTM,Dense,Dropout,Embedding,Bidirectional
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
import numpy as np
import string
import pickle
from sklearn.metrics import confusion_matrix
from keras import regularizers
import tensorflow as tf




def remove_stopwords(text):
    words = [word for word in text if word not in stopwords.words("english")]
    return words


def make_model():
    model = Sequential()
    model.add(Embedding(vocab_size,300,weights = [embedding_matrix],input_length=300,trainable = False))
    model.add(Bidirectional(LSTM(75)))
    model.add(Dropout(0.2))
    model.add(Dense(32,activation = 'relu'))
    model.add(Dropout(0.2))
    model.add(Dense(1,activation = 'sigmoid'))
    model.compile(optimizer='adam',loss='binary_crossentropy',metrics = ['accuracy'])
    history = model.fit(pad_seq,labels,epochs = 5,batch_size=256,validation_split=0.2)
    model.save("/content/gdrive/My Drive/ColabData/BILSTM_300_CPU_snopes_2.0")



def evaluate_model():

    #model = tf.keras.models.load_model('/content/gdrive/My Drive/ColabData/BILSTM_300_CPU_snopes_2.0')
    df = pd.read_csv('/content/gdrive/My Drive/ColabData/kaggle_train.csv')
    df = df.drop(columns=['id','author'])
    df = df.dropna()

    df['text'] = df['text'].apply(lambda x: tf.keras.preprocessing.text.text_to_word_sequence(x, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True, split=' '))
    df['text'] =df['text'].apply(lambda x: remove_stopwords(x))
    body_sequences = token.texts_to_sequences(df['text'])
    data = pad_sequences(body_sequences, maxlen=300)
    pred = model.predict(data)

    y_pred = np.zeros(np.shape(pred))

    for i in range(pred.shape[0]):
     if pred[i,0] >= 0.5:
        y_pred[i,0] = 1
     else:
        y_pred[i,0] = 0


    print(model.evaluate(data, df['label']))
    y_true = df['label']
    print(confusion_matrix(y_true, y_pred))




def main():


    #read merged files and processed file obtained from spark 
    df_merged = pd.read_csv("/content/gdrive/My Drive/ColabData/snopes_saved_merged_processed.csv")


    df_merged = df_merged.drop(columns=['Unnamed: 0'])

    #making embeddings using Glove 300 Dimensional vectors
    x = df_merged['text']
    embedding_vector = {}
    f = open('/content/gdrive/My Drive/ColabData/glove.6B.300d.txt')
    for line in tqdm(f):
        value = line.split(' ')
        word = value[0]
        coef = np.array(value[1:],dtype = 'float32')
        embedding_vector[word] = coef

    #opening saved tokenizer
    #with open('/content/gdrive/My Drive/ColabData/tokenizer_bilstm_snopes.pickle', 'rb') as handle:
    #  token = pickle.load(handle)

    #if not saved, create one 

    token = Tokenizer()
    token.fit_on_texts(x)

    #generating sequences and word indices based on the training data
    seq = token.texts_to_sequences(x)
    pad_seq = pad_sequences(seq,maxlen=300)
    embedding_matrix = np.zeros((vocab_size,300))
    for word,i in tqdm(token.word_index.items()):
        embedding_value = embedding_vector.get(word)
        if embedding_value is not None:
            embedding_matrix[i] = embedding_value

    vocab_size = len(token.word_index)+1
    print(vocab_size)
    labels = df_merged['label']


    

if __name__ == '__main__':
    main()



